{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "import torch.utils.data as data_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import time\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=pd.read_csv('../data/train.csv', index_col='ID_code')\n",
    "test=pd.read_csv('../data/test.csv', index_col='ID_code')\n",
    "submission=pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000, 200)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>var_0</th>\n",
       "      <th>var_1</th>\n",
       "      <th>var_2</th>\n",
       "      <th>var_3</th>\n",
       "      <th>var_4</th>\n",
       "      <th>var_5</th>\n",
       "      <th>var_6</th>\n",
       "      <th>var_7</th>\n",
       "      <th>var_8</th>\n",
       "      <th>var_9</th>\n",
       "      <th>...</th>\n",
       "      <th>var_190</th>\n",
       "      <th>var_191</th>\n",
       "      <th>var_192</th>\n",
       "      <th>var_193</th>\n",
       "      <th>var_194</th>\n",
       "      <th>var_195</th>\n",
       "      <th>var_196</th>\n",
       "      <th>var_197</th>\n",
       "      <th>var_198</th>\n",
       "      <th>var_199</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ID_code</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>train_0</th>\n",
       "      <td>8.9255</td>\n",
       "      <td>-6.7863</td>\n",
       "      <td>11.9081</td>\n",
       "      <td>5.0930</td>\n",
       "      <td>11.4607</td>\n",
       "      <td>-9.2834</td>\n",
       "      <td>5.1187</td>\n",
       "      <td>18.6266</td>\n",
       "      <td>-4.9200</td>\n",
       "      <td>5.7470</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4354</td>\n",
       "      <td>3.9642</td>\n",
       "      <td>3.1364</td>\n",
       "      <td>1.6910</td>\n",
       "      <td>18.5227</td>\n",
       "      <td>-2.3978</td>\n",
       "      <td>7.8784</td>\n",
       "      <td>8.5635</td>\n",
       "      <td>12.7803</td>\n",
       "      <td>-1.0914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_1</th>\n",
       "      <td>11.5006</td>\n",
       "      <td>-4.1473</td>\n",
       "      <td>13.8588</td>\n",
       "      <td>5.3890</td>\n",
       "      <td>12.3622</td>\n",
       "      <td>7.0433</td>\n",
       "      <td>5.6208</td>\n",
       "      <td>16.5338</td>\n",
       "      <td>3.1468</td>\n",
       "      <td>8.0851</td>\n",
       "      <td>...</td>\n",
       "      <td>7.6421</td>\n",
       "      <td>7.7214</td>\n",
       "      <td>2.5837</td>\n",
       "      <td>10.9516</td>\n",
       "      <td>15.4305</td>\n",
       "      <td>2.0339</td>\n",
       "      <td>8.1267</td>\n",
       "      <td>8.7889</td>\n",
       "      <td>18.3560</td>\n",
       "      <td>1.9518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_2</th>\n",
       "      <td>8.6093</td>\n",
       "      <td>-2.7457</td>\n",
       "      <td>12.0805</td>\n",
       "      <td>7.8928</td>\n",
       "      <td>10.5825</td>\n",
       "      <td>-9.0837</td>\n",
       "      <td>6.9427</td>\n",
       "      <td>14.6155</td>\n",
       "      <td>-4.9193</td>\n",
       "      <td>5.9525</td>\n",
       "      <td>...</td>\n",
       "      <td>2.9057</td>\n",
       "      <td>9.7905</td>\n",
       "      <td>1.6704</td>\n",
       "      <td>1.6858</td>\n",
       "      <td>21.6042</td>\n",
       "      <td>3.1417</td>\n",
       "      <td>-6.5213</td>\n",
       "      <td>8.2675</td>\n",
       "      <td>14.7222</td>\n",
       "      <td>0.3965</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_3</th>\n",
       "      <td>11.0604</td>\n",
       "      <td>-2.1518</td>\n",
       "      <td>8.9522</td>\n",
       "      <td>7.1957</td>\n",
       "      <td>12.5846</td>\n",
       "      <td>-1.8361</td>\n",
       "      <td>5.8428</td>\n",
       "      <td>14.9250</td>\n",
       "      <td>-5.8609</td>\n",
       "      <td>8.2450</td>\n",
       "      <td>...</td>\n",
       "      <td>4.4666</td>\n",
       "      <td>4.7433</td>\n",
       "      <td>0.7178</td>\n",
       "      <td>1.4214</td>\n",
       "      <td>23.0347</td>\n",
       "      <td>-1.2706</td>\n",
       "      <td>-2.9275</td>\n",
       "      <td>10.2922</td>\n",
       "      <td>17.9697</td>\n",
       "      <td>-8.9996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>train_4</th>\n",
       "      <td>9.8369</td>\n",
       "      <td>-1.4834</td>\n",
       "      <td>12.8746</td>\n",
       "      <td>6.6375</td>\n",
       "      <td>12.2772</td>\n",
       "      <td>2.4486</td>\n",
       "      <td>5.9405</td>\n",
       "      <td>19.2514</td>\n",
       "      <td>6.2654</td>\n",
       "      <td>7.6784</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.4905</td>\n",
       "      <td>9.5214</td>\n",
       "      <td>-0.1508</td>\n",
       "      <td>9.1942</td>\n",
       "      <td>13.2876</td>\n",
       "      <td>-1.5121</td>\n",
       "      <td>3.9267</td>\n",
       "      <td>9.5031</td>\n",
       "      <td>17.9974</td>\n",
       "      <td>-8.8104</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           var_0   var_1    var_2   var_3    var_4   var_5   var_6    var_7  \\\n",
       "ID_code                                                                       \n",
       "train_0   8.9255 -6.7863  11.9081  5.0930  11.4607 -9.2834  5.1187  18.6266   \n",
       "train_1  11.5006 -4.1473  13.8588  5.3890  12.3622  7.0433  5.6208  16.5338   \n",
       "train_2   8.6093 -2.7457  12.0805  7.8928  10.5825 -9.0837  6.9427  14.6155   \n",
       "train_3  11.0604 -2.1518   8.9522  7.1957  12.5846 -1.8361  5.8428  14.9250   \n",
       "train_4   9.8369 -1.4834  12.8746  6.6375  12.2772  2.4486  5.9405  19.2514   \n",
       "\n",
       "          var_8   var_9   ...     var_190  var_191  var_192  var_193  var_194  \\\n",
       "ID_code                   ...                                                   \n",
       "train_0 -4.9200  5.7470   ...      4.4354   3.9642   3.1364   1.6910  18.5227   \n",
       "train_1  3.1468  8.0851   ...      7.6421   7.7214   2.5837  10.9516  15.4305   \n",
       "train_2 -4.9193  5.9525   ...      2.9057   9.7905   1.6704   1.6858  21.6042   \n",
       "train_3 -5.8609  8.2450   ...      4.4666   4.7433   0.7178   1.4214  23.0347   \n",
       "train_4  6.2654  7.6784   ...     -1.4905   9.5214  -0.1508   9.1942  13.2876   \n",
       "\n",
       "         var_195  var_196  var_197  var_198  var_199  \n",
       "ID_code                                               \n",
       "train_0  -2.3978   7.8784   8.5635  12.7803  -1.0914  \n",
       "train_1   2.0339   8.1267   8.7889  18.3560   1.9518  \n",
       "train_2   3.1417  -6.5213   8.2675  14.7222   0.3965  \n",
       "train_3  -1.2706  -2.9275  10.2922  17.9697  -8.9996  \n",
       "train_4  -1.5121   3.9267   9.5031  17.9974  -8.8104  \n",
       "\n",
       "[5 rows x 200 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = train.loc[:,train.columns !='target']\n",
    "print(x.shape)\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200000,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ID_code\n",
       "train_0    0\n",
       "train_1    0\n",
       "train_2    0\n",
       "train_3    0\n",
       "train_4    0\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = train['target']\n",
    "print(y.shape)\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=scaler.fit_transform(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-1.1152668122349497e-15, 1.0000000000000013)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.mean(), x.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = scaler.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_splits = 5\n",
    "splits = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "splits=list(splits.split(x,y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(array([     1,      2,      3, ..., 199996, 199997, 199999]),\n",
       "  array([     0,     11,     12, ..., 199988, 199992, 199998])),\n",
       " (array([     0,      1,      2, ..., 199997, 199998, 199999]),\n",
       "  array([     4,     24,     32, ..., 199993, 199994, 199996])),\n",
       " (array([     0,      1,      2, ..., 199996, 199998, 199999]),\n",
       "  array([     3,      8,     15, ..., 199979, 199980, 199997]))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(splits))\n",
    "splits[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid = next(iter(splits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(159999, 40001)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train), len(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.optimizer import Optimizer\n",
    "\n",
    "class CyclicLR(object):\n",
    "\n",
    "    def __init__(self, optimizer, base_lr=1e-3, max_lr=6e-3,\n",
    "                 step_size=2000, mode='triangular', gamma=1.,\n",
    "                 scale_fn=None, scale_mode='cycle', last_batch_iteration=-1):\n",
    "\n",
    "        if not isinstance(optimizer, Optimizer):\n",
    "            raise TypeError('{} is not an Optimizer'.format(\n",
    "                type(optimizer).__name__))\n",
    "        self.optimizer = optimizer\n",
    "\n",
    "        if isinstance(base_lr, list) or isinstance(base_lr, tuple):\n",
    "            if len(base_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} base_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(base_lr)))\n",
    "            self.base_lrs = list(base_lr)\n",
    "        else:\n",
    "            self.base_lrs = [base_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        if isinstance(max_lr, list) or isinstance(max_lr, tuple):\n",
    "            if len(max_lr) != len(optimizer.param_groups):\n",
    "                raise ValueError(\"expected {} max_lr, got {}\".format(\n",
    "                    len(optimizer.param_groups), len(max_lr)))\n",
    "            self.max_lrs = list(max_lr)\n",
    "        else:\n",
    "            self.max_lrs = [max_lr] * len(optimizer.param_groups)\n",
    "\n",
    "        self.step_size = step_size\n",
    "\n",
    "        if mode not in ['triangular', 'triangular2', 'exp_range'] \\\n",
    "                and scale_fn is None:\n",
    "            raise ValueError('mode is invalid and scale_fn is None')\n",
    "\n",
    "        self.mode = mode\n",
    "        self.gamma = gamma\n",
    "\n",
    "        if scale_fn is None:\n",
    "            if self.mode == 'triangular':\n",
    "                self.scale_fn = self._triangular_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'triangular2':\n",
    "                self.scale_fn = self._triangular2_scale_fn\n",
    "                self.scale_mode = 'cycle'\n",
    "            elif self.mode == 'exp_range':\n",
    "                self.scale_fn = self._exp_range_scale_fn\n",
    "                self.scale_mode = 'iterations'\n",
    "        else:\n",
    "            self.scale_fn = scale_fn\n",
    "            self.scale_mode = scale_mode\n",
    "\n",
    "        self.batch_step(last_batch_iteration + 1)\n",
    "        self.last_batch_iteration = last_batch_iteration\n",
    "\n",
    "    def batch_step(self, batch_iteration=None):\n",
    "        if batch_iteration is None:\n",
    "            batch_iteration = self.last_batch_iteration + 1\n",
    "        self.last_batch_iteration = batch_iteration\n",
    "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "    def _triangular_scale_fn(self, x):\n",
    "        return 1.\n",
    "\n",
    "    def _triangular2_scale_fn(self, x):\n",
    "        return 1 / (2. ** (x - 1))\n",
    "\n",
    "    def _exp_range_scale_fn(self, x):\n",
    "        return self.gamma**(x)\n",
    "\n",
    "    def get_lr(self):\n",
    "        step_size = float(self.step_size)\n",
    "        cycle = np.floor(1 + self.last_batch_iteration / (2 * step_size))\n",
    "        x = np.abs(self.last_batch_iteration / step_size - 2 * cycle + 1)\n",
    "\n",
    "        lrs = []\n",
    "        param_lrs = zip(self.optimizer.param_groups, self.base_lrs, self.max_lrs)\n",
    "        for param_group, base_lr, max_lr in param_lrs:\n",
    "            base_height = (max_lr - base_lr) * np.maximum(0, (1 - x))\n",
    "            if self.scale_mode == 'cycle':\n",
    "                lr = base_lr + base_height * self.scale_fn(cycle)\n",
    "            else:\n",
    "                lr = base_lr + base_height * self.scale_fn(self.last_batch_iteration)\n",
    "            lrs.append(lr)\n",
    "        return lrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Simple_NN(nn.Module):\n",
    "    def __init__(self ,input_dim ,hidden_dim, dropout = 0.75):\n",
    "        super(Simple_NN, self).__init__()\n",
    "        \n",
    "        self.inpt_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc1 = nn.Linear(1, hidden_dim)\n",
    "        self.fc2 = nn.Linear(int(hidden_dim*input_dim), 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        #self.fc3 = nn.Linear(int(hidden_dim/2*input_dim), int(hidden_dim/4))\n",
    "        #self.fc4 = nn.Linear(int(hidden_dim/4*input_dim), int(hidden_dim/8))\n",
    "        #self.fc5 = nn.Linear(int(hidden_dim/8*input_dim), 1)\n",
    "        #self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        #self.bn2 = nn.BatchNorm1d(int(hidden_dim/2))\n",
    "        #self.bn3 = nn.BatchNorm1d(int(hidden_dim/4))\n",
    "        #self.bn4 = nn.BatchNorm1d(int(hidden_dim/8))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        b_size = x.size(0) #256, batch_size 256\n",
    "        x = x.view(-1, 1) #(256 X 200, 1)\n",
    "        y = self.fc1(x) #(51200, 16)\n",
    "        y = self.relu(y) #(51200, 16)\n",
    "        y = y.view(b_size, -1)#(256, 51200 X 16 / 256)\n",
    "        \n",
    "        y= self.fc2(y) #(256, 1)\n",
    "        out = self.sigmoid(y)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model =Simple_NN(200,16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Loss function\n",
    "#loss_fn = FocalLoss(2)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001,weight_decay=1e-5) \n",
    "# Using Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ì•„ëž˜ Cycling learning rate ë¶€í„° ìž¬ ì„¤ì • í•„ìš”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/40 \t loss=0.2431 \t val_loss=0.2169 \t time=12.45s\n",
      "Epoch 2/40 \t loss=0.2183 \t val_loss=0.2121 \t time=12.62s\n",
      "Epoch 3/40 \t loss=0.2161 \t val_loss=0.2142 \t time=13.06s\n",
      "Epoch 4/40 \t loss=0.2120 \t val_loss=0.2129 \t time=12.98s\n",
      "Epoch 5/40 \t loss=0.2086 \t val_loss=0.2072 \t time=13.08s\n",
      "Epoch 6/40 \t loss=0.2071 \t val_loss=0.2055 \t time=12.93s\n",
      "Epoch 7/40 \t loss=0.2037 \t val_loss=0.2062 \t time=12.98s\n",
      "Epoch 8/40 \t loss=0.2044 \t val_loss=0.2089 \t time=12.95s\n",
      "Epoch 9/40 \t loss=0.2068 \t val_loss=0.2055 \t time=13.06s\n",
      "Epoch 10/40 \t loss=0.2079 \t val_loss=0.2100 \t time=13.72s\n",
      "Epoch 11/40 \t loss=0.2035 \t val_loss=0.2041 \t time=13.08s\n",
      "Epoch 12/40 \t loss=0.2024 \t val_loss=0.2033 \t time=13.14s\n",
      "Epoch 13/40 \t loss=0.2007 \t val_loss=0.2069 \t time=13.21s\n",
      "Epoch 14/40 \t loss=0.2006 \t val_loss=0.2037 \t time=13.16s\n",
      "Epoch 15/40 \t loss=0.2026 \t val_loss=0.2064 \t time=12.72s\n",
      "Epoch 16/40 \t loss=0.2034 \t val_loss=0.2038 \t time=12.73s\n",
      "Epoch 17/40 \t loss=0.2025 \t val_loss=0.2035 \t time=12.76s\n",
      "Epoch 18/40 \t loss=0.2010 \t val_loss=0.2035 \t time=12.85s\n",
      "Epoch 19/40 \t loss=0.1999 \t val_loss=0.2029 \t time=12.97s\n",
      "Epoch 20/40 \t loss=0.1993 \t val_loss=0.2033 \t time=12.74s\n",
      "Epoch 21/40 \t loss=0.2004 \t val_loss=0.2056 \t time=12.69s\n",
      "Epoch 22/40 \t loss=0.2017 \t val_loss=0.2064 \t time=13.06s\n",
      "Epoch 23/40 \t loss=0.2011 \t val_loss=0.2106 \t time=13.47s\n",
      "Epoch 24/40 \t loss=0.2006 \t val_loss=0.2030 \t time=13.38s\n",
      "Epoch 25/40 \t loss=0.1997 \t val_loss=0.2042 \t time=12.85s\n",
      "Epoch 26/40 \t loss=0.1986 \t val_loss=0.2031 \t time=12.94s\n",
      "Epoch 27/40 \t loss=0.1991 \t val_loss=0.2036 \t time=12.94s\n",
      "Epoch 28/40 \t loss=0.1996 \t val_loss=0.2036 \t time=13.03s\n",
      "Epoch 29/40 \t loss=0.2004 \t val_loss=0.2053 \t time=12.79s\n",
      "Epoch 30/40 \t loss=0.2002 \t val_loss=0.2029 \t time=12.91s\n",
      "Epoch 31/40 \t loss=0.1994 \t val_loss=0.2028 \t time=12.94s\n",
      "Epoch 32/40 \t loss=0.1985 \t val_loss=0.2027 \t time=12.93s\n",
      "Epoch 33/40 \t loss=0.1983 \t val_loss=0.2033 \t time=13.12s\n",
      "Epoch 34/40 \t loss=0.1993 \t val_loss=0.2032 \t time=12.82s\n",
      "Epoch 35/40 \t loss=0.1996 \t val_loss=0.2039 \t time=12.94s\n",
      "Epoch 36/40 \t loss=0.1995 \t val_loss=0.2047 \t time=13.00s\n",
      "Epoch 37/40 \t loss=0.1986 \t val_loss=0.2033 \t time=13.38s\n",
      "Epoch 38/40 \t loss=0.1983 \t val_loss=0.2029 \t time=13.15s\n",
      "Epoch 39/40 \t loss=0.1979 \t val_loss=0.2037 \t time=13.22s\n",
      "Epoch 40/40 \t loss=0.1983 \t val_loss=0.2046 \t time=12.83s\n",
      "Fold 2\n",
      "Epoch 1/40 \t loss=0.2378 \t val_loss=0.2220 \t time=13.34s\n",
      "Epoch 2/40 \t loss=0.2170 \t val_loss=0.2134 \t time=13.54s\n",
      "Epoch 3/40 \t loss=0.2137 \t val_loss=0.2146 \t time=13.13s\n",
      "Epoch 4/40 \t loss=0.2110 \t val_loss=0.2097 \t time=13.23s\n",
      "Epoch 5/40 \t loss=0.2068 \t val_loss=0.2082 \t time=13.46s\n",
      "Epoch 6/40 \t loss=0.2045 \t val_loss=0.2087 \t time=12.90s\n",
      "Epoch 7/40 \t loss=0.2031 \t val_loss=0.2081 \t time=13.11s\n",
      "Epoch 8/40 \t loss=0.2041 \t val_loss=0.2175 \t time=12.85s\n",
      "Epoch 9/40 \t loss=0.2049 \t val_loss=0.2105 \t time=13.21s\n",
      "Epoch 10/40 \t loss=0.2061 \t val_loss=0.2094 \t time=13.22s\n",
      "Epoch 11/40 \t loss=0.2029 \t val_loss=0.2064 \t time=13.34s\n",
      "Epoch 12/40 \t loss=0.2020 \t val_loss=0.2051 \t time=12.99s\n",
      "Epoch 13/40 \t loss=0.2011 \t val_loss=0.2065 \t time=12.98s\n",
      "Epoch 14/40 \t loss=0.2006 \t val_loss=0.2048 \t time=12.83s\n",
      "Epoch 15/40 \t loss=0.2025 \t val_loss=0.2088 \t time=13.02s\n",
      "Epoch 16/40 \t loss=0.2028 \t val_loss=0.2099 \t time=13.10s\n",
      "Epoch 17/40 \t loss=0.2023 \t val_loss=0.2048 \t time=13.13s\n",
      "Epoch 18/40 \t loss=0.2008 \t val_loss=0.2060 \t time=13.17s\n",
      "Epoch 19/40 \t loss=0.2006 \t val_loss=0.2052 \t time=13.09s\n",
      "Epoch 20/40 \t loss=0.1988 \t val_loss=0.2056 \t time=13.02s\n",
      "Epoch 21/40 \t loss=0.2000 \t val_loss=0.2056 \t time=13.22s\n",
      "Epoch 22/40 \t loss=0.2004 \t val_loss=0.2051 \t time=12.98s\n",
      "Epoch 23/40 \t loss=0.2015 \t val_loss=0.2070 \t time=13.07s\n",
      "Epoch 24/40 \t loss=0.2004 \t val_loss=0.2068 \t time=12.92s\n",
      "Epoch 25/40 \t loss=0.1991 \t val_loss=0.2045 \t time=13.19s\n",
      "Epoch 26/40 \t loss=0.1983 \t val_loss=0.2042 \t time=12.93s\n",
      "Epoch 27/40 \t loss=0.1986 \t val_loss=0.2047 \t time=13.00s\n",
      "Epoch 28/40 \t loss=0.1995 \t val_loss=0.2055 \t time=13.16s\n",
      "Epoch 29/40 \t loss=0.1999 \t val_loss=0.2054 \t time=13.11s\n",
      "Epoch 30/40 \t loss=0.1994 \t val_loss=0.2047 \t time=13.41s\n",
      "Epoch 31/40 \t loss=0.1989 \t val_loss=0.2045 \t time=13.18s\n",
      "Epoch 32/40 \t loss=0.1981 \t val_loss=0.2053 \t time=13.20s\n",
      "Epoch 33/40 \t loss=0.1976 \t val_loss=0.2082 \t time=12.97s\n",
      "Epoch 34/40 \t loss=0.1984 \t val_loss=0.2047 \t time=13.02s\n",
      "Epoch 35/40 \t loss=0.1989 \t val_loss=0.2079 \t time=13.04s\n",
      "Epoch 36/40 \t loss=0.1997 \t val_loss=0.2060 \t time=12.96s\n",
      "Epoch 37/40 \t loss=0.1984 \t val_loss=0.2048 \t time=13.32s\n",
      "Epoch 38/40 \t loss=0.1980 \t val_loss=0.2049 \t time=13.79s\n",
      "Epoch 39/40 \t loss=0.1974 \t val_loss=0.2066 \t time=13.35s\n",
      "Epoch 40/40 \t loss=0.1978 \t val_loss=0.2045 \t time=13.08s\n",
      "Fold 3\n",
      "Epoch 1/40 \t loss=0.2362 \t val_loss=0.2067 \t time=13.12s\n",
      "Epoch 2/40 \t loss=0.2137 \t val_loss=0.2046 \t time=13.24s\n",
      "Epoch 3/40 \t loss=0.2138 \t val_loss=0.2035 \t time=13.93s\n",
      "Epoch 4/40 \t loss=0.2100 \t val_loss=0.2080 \t time=13.40s\n",
      "Epoch 5/40 \t loss=0.2065 \t val_loss=0.2019 \t time=13.54s\n",
      "Epoch 6/40 \t loss=0.2044 \t val_loss=0.2012 \t time=13.37s\n",
      "Epoch 7/40 \t loss=0.2024 \t val_loss=0.2015 \t time=13.34s\n",
      "Epoch 8/40 \t loss=0.2039 \t val_loss=0.2019 \t time=13.60s\n",
      "Epoch 9/40 \t loss=0.2049 \t val_loss=0.2009 \t time=13.49s\n",
      "Epoch 10/40 \t loss=0.2060 \t val_loss=0.2017 \t time=13.30s\n",
      "Epoch 11/40 \t loss=0.2048 \t val_loss=0.2002 \t time=13.18s\n",
      "Epoch 12/40 \t loss=0.2025 \t val_loss=0.2035 \t time=13.53s\n",
      "Epoch 13/40 \t loss=0.2011 \t val_loss=0.1994 \t time=13.40s\n",
      "Epoch 14/40 \t loss=0.2014 \t val_loss=0.2005 \t time=13.39s\n",
      "Epoch 15/40 \t loss=0.2027 \t val_loss=0.1997 \t time=13.19s\n",
      "Epoch 16/40 \t loss=0.2041 \t val_loss=0.2007 \t time=13.19s\n",
      "Epoch 17/40 \t loss=0.2039 \t val_loss=0.2011 \t time=13.29s\n",
      "Epoch 18/40 \t loss=0.2021 \t val_loss=0.2006 \t time=13.25s\n",
      "Epoch 19/40 \t loss=0.2006 \t val_loss=0.2017 \t time=13.37s\n",
      "Epoch 20/40 \t loss=0.2002 \t val_loss=0.2015 \t time=13.26s\n",
      "Epoch 21/40 \t loss=0.2011 \t val_loss=0.2007 \t time=13.50s\n",
      "Epoch 22/40 \t loss=0.2017 \t val_loss=0.2032 \t time=13.38s\n",
      "Epoch 23/40 \t loss=0.2023 \t val_loss=0.1999 \t time=13.30s\n",
      "Epoch 24/40 \t loss=0.2018 \t val_loss=0.2010 \t time=14.01s\n",
      "Epoch 25/40 \t loss=0.2006 \t val_loss=0.1991 \t time=13.43s\n",
      "Epoch 26/40 \t loss=0.1996 \t val_loss=0.1994 \t time=13.62s\n",
      "Epoch 27/40 \t loss=0.2002 \t val_loss=0.1999 \t time=13.37s\n",
      "Epoch 28/40 \t loss=0.2008 \t val_loss=0.1997 \t time=13.39s\n",
      "Epoch 29/40 \t loss=0.2012 \t val_loss=0.1999 \t time=13.36s\n",
      "Epoch 30/40 \t loss=0.2012 \t val_loss=0.2021 \t time=13.26s\n",
      "Epoch 31/40 \t loss=0.2004 \t val_loss=0.1993 \t time=13.35s\n",
      "Epoch 32/40 \t loss=0.1997 \t val_loss=0.1992 \t time=13.34s\n",
      "Epoch 33/40 \t loss=0.1995 \t val_loss=0.1992 \t time=13.12s\n",
      "Epoch 34/40 \t loss=0.1996 \t val_loss=0.1997 \t time=13.20s\n",
      "Epoch 35/40 \t loss=0.2003 \t val_loss=0.2003 \t time=13.68s\n",
      "Epoch 36/40 \t loss=0.2008 \t val_loss=0.2001 \t time=13.49s\n",
      "Epoch 37/40 \t loss=0.1999 \t val_loss=0.1992 \t time=13.30s\n",
      "Epoch 38/40 \t loss=0.1993 \t val_loss=0.2010 \t time=13.11s\n",
      "Epoch 39/40 \t loss=0.1990 \t val_loss=0.2019 \t time=13.13s\n",
      "Epoch 40/40 \t loss=0.1993 \t val_loss=0.1998 \t time=13.36s\n",
      "Fold 4\n",
      "Epoch 1/40 \t loss=0.2397 \t val_loss=0.2157 \t time=13.67s\n",
      "Epoch 2/40 \t loss=0.2198 \t val_loss=0.2133 \t time=13.19s\n",
      "Epoch 3/40 \t loss=0.2142 \t val_loss=0.2093 \t time=13.15s\n",
      "Epoch 4/40 \t loss=0.2112 \t val_loss=0.2073 \t time=13.32s\n",
      "Epoch 5/40 \t loss=0.2073 \t val_loss=0.2072 \t time=13.32s\n",
      "Epoch 6/40 \t loss=0.2044 \t val_loss=0.2045 \t time=13.67s\n",
      "Epoch 7/40 \t loss=0.2025 \t val_loss=0.2112 \t time=13.26s\n",
      "Epoch 8/40 \t loss=0.2048 \t val_loss=0.2634 \t time=13.90s\n",
      "Epoch 9/40 \t loss=0.2067 \t val_loss=0.2098 \t time=13.39s\n",
      "Epoch 10/40 \t loss=0.2070 \t val_loss=0.2064 \t time=13.37s\n",
      "Epoch 11/40 \t loss=0.2041 \t val_loss=0.2061 \t time=13.28s\n",
      "Epoch 12/40 \t loss=0.2026 \t val_loss=0.2032 \t time=13.76s\n",
      "Epoch 13/40 \t loss=0.2012 \t val_loss=0.2038 \t time=13.48s\n",
      "Epoch 14/40 \t loss=0.2011 \t val_loss=0.2038 \t time=13.37s\n",
      "Epoch 15/40 \t loss=0.2033 \t val_loss=0.2042 \t time=13.33s\n",
      "Epoch 16/40 \t loss=0.2050 \t val_loss=0.2508 \t time=13.24s\n",
      "Epoch 17/40 \t loss=0.2085 \t val_loss=0.2051 \t time=13.49s\n",
      "Epoch 18/40 \t loss=0.2014 \t val_loss=0.2034 \t time=13.37s\n",
      "Epoch 19/40 \t loss=0.2002 \t val_loss=0.2043 \t time=13.26s\n",
      "Epoch 20/40 \t loss=0.1999 \t val_loss=0.2098 \t time=13.01s\n",
      "Epoch 21/40 \t loss=0.2006 \t val_loss=0.2078 \t time=13.02s\n",
      "Epoch 22/40 \t loss=0.2013 \t val_loss=0.2236 \t time=13.33s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/40 \t loss=0.2019 \t val_loss=0.2106 \t time=13.11s\n",
      "Epoch 24/40 \t loss=0.2014 \t val_loss=0.2035 \t time=13.12s\n",
      "Epoch 25/40 \t loss=0.2024 \t val_loss=0.2184 \t time=13.20s\n",
      "Epoch 26/40 \t loss=0.2026 \t val_loss=0.2037 \t time=13.12s\n",
      "Epoch 27/40 \t loss=0.1991 \t val_loss=0.2166 \t time=13.35s\n",
      "Epoch 28/40 \t loss=0.2001 \t val_loss=0.2042 \t time=13.06s\n",
      "Epoch 29/40 \t loss=0.2000 \t val_loss=0.2035 \t time=13.06s\n",
      "Epoch 30/40 \t loss=0.1994 \t val_loss=0.2031 \t time=13.05s\n",
      "Epoch 31/40 \t loss=0.1989 \t val_loss=0.2034 \t time=13.30s\n",
      "Epoch 32/40 \t loss=0.1990 \t val_loss=0.2051 \t time=13.13s\n",
      "Epoch 33/40 \t loss=0.1987 \t val_loss=0.2045 \t time=13.30s\n",
      "Epoch 34/40 \t loss=0.1989 \t val_loss=0.2032 \t time=13.32s\n",
      "Epoch 35/40 \t loss=0.1988 \t val_loss=0.2036 \t time=13.21s\n",
      "Epoch 36/40 \t loss=0.1993 \t val_loss=0.2038 \t time=13.73s\n",
      "Epoch 37/40 \t loss=0.1985 \t val_loss=0.2032 \t time=14.13s\n",
      "Epoch 38/40 \t loss=0.1982 \t val_loss=0.2049 \t time=13.45s\n",
      "Epoch 39/40 \t loss=0.1978 \t val_loss=0.2065 \t time=15.89s\n",
      "Epoch 40/40 \t loss=0.1976 \t val_loss=0.2034 \t time=17.20s\n",
      "Fold 5\n",
      "Epoch 1/40 \t loss=0.2377 \t val_loss=0.2130 \t time=15.08s\n",
      "Epoch 2/40 \t loss=0.2142 \t val_loss=0.2314 \t time=15.09s\n",
      "Epoch 3/40 \t loss=0.2117 \t val_loss=0.2242 \t time=13.88s\n",
      "Epoch 4/40 \t loss=0.2116 \t val_loss=0.2640 \t time=14.61s\n",
      "Epoch 5/40 \t loss=0.2062 \t val_loss=0.2160 \t time=14.71s\n",
      "Epoch 6/40 \t loss=0.2034 \t val_loss=0.2097 \t time=14.26s\n",
      "Epoch 7/40 \t loss=0.2010 \t val_loss=0.2108 \t time=15.17s\n",
      "Epoch 8/40 \t loss=0.2035 \t val_loss=0.2074 \t time=14.31s\n",
      "Epoch 9/40 \t loss=0.2047 \t val_loss=0.2075 \t time=15.37s\n",
      "Epoch 10/40 \t loss=0.2055 \t val_loss=0.2067 \t time=15.81s\n",
      "Epoch 11/40 \t loss=0.2032 \t val_loss=0.2256 \t time=15.52s\n",
      "Epoch 12/40 \t loss=0.2022 \t val_loss=0.2057 \t time=15.89s\n",
      "Epoch 13/40 \t loss=0.2000 \t val_loss=0.2141 \t time=14.27s\n",
      "Epoch 14/40 \t loss=0.2003 \t val_loss=0.2062 \t time=13.94s\n",
      "Epoch 15/40 \t loss=0.2018 \t val_loss=0.2084 \t time=16.44s\n",
      "Epoch 16/40 \t loss=0.2025 \t val_loss=0.2096 \t time=16.71s\n",
      "Epoch 17/40 \t loss=0.2033 \t val_loss=0.2069 \t time=13.57s\n",
      "Epoch 18/40 \t loss=0.2009 \t val_loss=0.2065 \t time=13.88s\n",
      "Epoch 19/40 \t loss=0.1995 \t val_loss=0.2056 \t time=13.85s\n",
      "Epoch 20/40 \t loss=0.1987 \t val_loss=0.2063 \t time=15.24s\n",
      "Epoch 21/40 \t loss=0.2014 \t val_loss=0.2202 \t time=13.78s\n",
      "Epoch 22/40 \t loss=0.2037 \t val_loss=0.2129 \t time=13.36s\n",
      "Epoch 23/40 \t loss=0.2039 \t val_loss=0.2557 \t time=15.03s\n",
      "Epoch 24/40 \t loss=0.2033 \t val_loss=0.2062 \t time=14.00s\n",
      "Epoch 25/40 \t loss=0.1993 \t val_loss=0.2101 \t time=13.19s\n",
      "Epoch 26/40 \t loss=0.2000 \t val_loss=0.2057 \t time=13.30s\n",
      "Epoch 27/40 \t loss=0.1996 \t val_loss=0.2059 \t time=13.28s\n",
      "Epoch 28/40 \t loss=0.1992 \t val_loss=0.2060 \t time=13.73s\n",
      "Epoch 29/40 \t loss=0.1994 \t val_loss=0.2054 \t time=13.94s\n",
      "Epoch 30/40 \t loss=0.1993 \t val_loss=0.2060 \t time=13.15s\n",
      "Epoch 31/40 \t loss=0.1986 \t val_loss=0.2064 \t time=13.11s\n",
      "Epoch 32/40 \t loss=0.1980 \t val_loss=0.2212 \t time=13.31s\n",
      "Epoch 33/40 \t loss=0.1982 \t val_loss=0.2058 \t time=13.42s\n",
      "Epoch 34/40 \t loss=0.1985 \t val_loss=0.2143 \t time=13.12s\n",
      "Epoch 35/40 \t loss=0.1991 \t val_loss=0.2073 \t time=12.96s\n",
      "Epoch 36/40 \t loss=0.2003 \t val_loss=0.2165 \t time=13.03s\n",
      "Epoch 37/40 \t loss=0.2002 \t val_loss=0.2057 \t time=13.00s\n",
      "Epoch 38/40 \t loss=0.1980 \t val_loss=0.2057 \t time=13.25s\n",
      "Epoch 39/40 \t loss=0.1975 \t val_loss=0.2073 \t time=13.20s\n",
      "Epoch 40/40 \t loss=0.1977 \t val_loss=0.2063 \t time=13.19s\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_target' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-26c42882cdf6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mtest_preds\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mtest_preds_fold\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msplits\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m \u001b[0mauc\u001b[0m  \u001b[1;33m=\u001b[0m  \u001b[0mround\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrain_preds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_losses_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mavg_val_losses_f\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mauc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_target' is not defined"
     ]
    }
   ],
   "source": [
    "## Hyperparameter\n",
    "n_epochs = 40\n",
    "batch_size = 256\n",
    "\n",
    "## Build tensor data for torch\n",
    "train_preds = np.zeros((len(x)))\n",
    "test_preds = np.zeros((len(test)))\n",
    "\n",
    "x_test = np.array(test)\n",
    "x_test= torch.tensor(x_test, dtype=torch.float)\n",
    "test = torch.utils.data.TensorDataset(x_test)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "avg_losses_f = []\n",
    "avg_val_losses_f = []\n",
    "\n",
    "## Start K-fold validation\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):  \n",
    "    x_train = np.array(x)\n",
    "    y_train = np.array(y)\n",
    "    \n",
    "    x_train_fold = torch.tensor(x_train[train_idx.astype(int)], dtype=torch.float)\n",
    "    y_train_fold = torch.tensor(y_train[train_idx.astype(int), np.newaxis], dtype=torch.float32)\n",
    "    \n",
    "    x_val_fold = torch.tensor(x_train[valid_idx.astype(int)], dtype=torch.float)\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx.astype(int), np.newaxis], dtype=torch.float32)\n",
    "    \n",
    "    \n",
    "    ######################Cycling learning rate########################\n",
    "    step_size = 2000\n",
    "    base_lr, max_lr = 0.001, 0.005  \n",
    "    optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), \n",
    "                                lr=max_lr)\n",
    "    \n",
    "    scheduler = CyclicLR(optimizer, \n",
    "                         base_lr=base_lr, \n",
    "                         max_lr=max_lr,\n",
    "                         step_size=step_size, \n",
    "                         mode='exp_range',\n",
    "                         gamma=0.99994\n",
    "                )\n",
    "    ###################################################################\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    for epoch in range(n_epochs):\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.\n",
    "        #avg_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(train_loader):\n",
    "            y_pred = model(x_batch)\n",
    "            ###################tuning learning rate###############\n",
    "            if scheduler:\n",
    "                print('cycle_LR')\n",
    "                scheduler.batch_step()\n",
    "\n",
    "            ######################################################\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item()/len(train_loader)\n",
    "            #avg_auc += round(roc_auc_score(y_batch.cpu(),y_pred.detach().cpu()),4) / len(train_loader)\n",
    "        model.eval()\n",
    "        \n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        #avg_val_auc = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            #avg_val_auc += round(roc_auc_score(y_batch.cpu(),sigmoid(y_pred.cpu().numpy())[:, 0]),4) / len(valid_loader)\n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "            \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    avg_losses_f.append(avg_loss)\n",
    "    avg_val_losses_f.append(avg_val_loss) \n",
    "    \n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)\n",
    "\n",
    "auc  =  round(roc_auc_score(y,train_preds),4)      \n",
    "print('All \\t loss={:.4f} \\t val_loss={:.4f} \\t auc={:.4f}'.format(np.average(avg_losses_f),np.average(avg_val_losses_f),auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "\n",
    "Now that the model is trained, we can use it for inference. We've done this before, but now we need to remember to set the model in inference mode with `model.eval()`. You'll also want to turn off autograd with the `torch.no_grad()` context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x196ffbece10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-2f4c830b81a5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-17-b683d5dd8fa4>\u001b[0m in \u001b[0;36msigmoid\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msigmoid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36m__array__\u001b[1;34m(self, dtype)\u001b[0m\n\u001b[0;32m    395\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__array__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 397\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    398\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead."
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "output = model(x_test)\n",
    "output=sigmoid(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output=output.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission['target'] = output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(submission.shape)\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('../submission/submission_nn_190312.csv', index=False, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
